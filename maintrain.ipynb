{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6878499,"sourceType":"datasetVersion","datasetId":3952124}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***Importing and treating CIC-DDoS-2019¶***","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.utils import resample\nfrom sklearn import preprocessing\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:10:18.267508Z","iopub.execute_input":"2025-05-11T12:10:18.267899Z","iopub.status.idle":"2025-05-11T12:10:18.868644Z","shell.execute_reply.started":"2025-05-11T12:10:18.267866Z","shell.execute_reply":"2025-05-11T12:10:18.867642Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DOWNSAMPLING AND BALANCING","metadata":{"execution":{"iopub.status.busy":"2025-03-12T10:05:17.921810Z","iopub.execute_input":"2025-03-12T10:05:17.922481Z","iopub.status.idle":"2025-03-12T10:05:17.945053Z","shell.execute_reply.started":"2025-03-12T10:05:17.922446Z","shell.execute_reply":"2025-03-12T10:05:17.939686Z"}}},{"cell_type":"code","source":"mult = 5\nnum_samples = 5000  # Variable to control the number of rows to read\n\ndef load_file(path):\n    # Count total lines in the file (excluding header)\n    total_lines = sum(1 for _ in open(path)) - 1  \n\n    # Ensure at most `num_samples` rows are read randomly\n    if total_lines > num_samples:\n        skip_rows = sorted(random.sample(range(1, total_lines + 1), total_lines - num_samples))\n    else:\n        skip_rows = None  # Read entire file if it's smaller than `num_samples`\n\n    data = pd.read_csv(path, skiprows=skip_rows, sep=',', low_memory=False)\n\n    is_benign = data[' Label'] == 'BENIGN'\n    flows_ok = data[is_benign]\n    flows_ddos_full = data[~is_benign]\n    \n    sizeDownSample = len(flows_ok) * mult  # Target size for anomalous data\n    \n    # Downsample majority class\n    if sizeDownSample < len(flows_ddos_full): \n        flows_ddos_reduced = resample(\n            flows_ddos_full,\n            replace=False,\n            n_samples=sizeDownSample,\n            random_state=27\n        )\n    else:\n        flows_ddos_reduced = flows_ddos_full\n\n    final_df = pd.concat([flows_ok, flows_ddos_reduced])\n\n    return final_df\n\n\ndef load_huge_file(path):\n    total_lines = sum(1 for _ in open(path)) - 1  \n\n    if total_lines > num_samples:\n        skip_rows = sorted(random.sample(range(1, total_lines + 1), total_lines - num_samples))\n    else:\n        skip_rows = None  \n\n    df_chunk = pd.read_csv(path, skiprows=skip_rows, chunksize=500000, low_memory=False)\n    \n    chunk_list_ok = []  \n    chunk_list_ddos = [] \n\n    for chunk in df_chunk:  \n        is_benign = chunk[' Label'] == 'BENIGN'\n        flows_ok = chunk[is_benign]\n        flows_ddos_full = chunk[~is_benign]\n        \n        if (len(flows_ok) * mult) < len(flows_ddos_full): \n            sizeDownSample = len(flows_ok) * mult  \n            \n            flows_ddos_reduced = resample(\n                flows_ddos_full,\n                replace=False,\n                n_samples=sizeDownSample,\n                random_state=27\n            )\n        else:\n            flows_ddos_reduced = flows_ddos_full\n            \n        chunk_list_ok.append(flows_ok)\n        chunk_list_ddos.append(flows_ddos_reduced)\n        \n    flows_ok = pd.concat(chunk_list_ok)\n    flows_ddos = pd.concat(chunk_list_ddos)\n\n    final_df = pd.concat([flows_ok, flows_ddos])\n\n    return final_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:10:18.869847Z","iopub.execute_input":"2025-05-11T12:10:18.870383Z","iopub.status.idle":"2025-05-11T12:10:18.880944Z","shell.execute_reply.started":"2025-05-11T12:10:18.870348Z","shell.execute_reply":"2025-05-11T12:10:18.879813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loading CIC-DDoS2019**","metadata":{}},{"cell_type":"code","source":"# Load first file\nflows = load_huge_file('/kaggle/input/cic-ddos2019-30gb-full-dataset-csv-files/01-12/TFTP.csv')\nprint('file 1 loaded')\n\n# List of remaining files\nfiles = [\n    \"DrDoS_LDAP.csv\", \"DrDoS_MSSQL.csv\", \"DrDoS_NetBIOS.csv\",\n    \"DrDoS_NTP.csv\", \"DrDoS_SNMP.csv\", \"DrDoS_SSDP.csv\",\n    \"DrDoS_UDP.csv\", \"Syn.csv\", \"DrDoS_DNS.csv\", \"UDPLag.csv\"\n]\n\n# Process each file\nfor i, file in enumerate(files, start=2):\n    df = load_file(f'/kaggle/input/cic-ddos2019-30gb-full-dataset-csv-files/01-12/{file}')\n    \n    # Concatenate new file data\n    flows = pd.concat([flows, df], ignore_index=True)\n    \n    print(f'file {i} loaded')\n\n# Save to CSV\nflows.to_csv('/kaggle/working/export_dataframe.csv', index=False, header=True)\n\n# Delete large variable\ndel flows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:10:18.882974Z","iopub.execute_input":"2025-05-11T12:10:18.883330Z","iopub.status.idle":"2025-05-11T12:18:33.886343Z","shell.execute_reply.started":"2025-05-11T12:10:18.883296Z","shell.execute_reply":"2025-05-11T12:18:33.884962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = \"/kaggle/input/cic-ddos2019-30gb-full-dataset-csv-files/03-11/\"\nfiles = [\"LDAP.csv\", \"MSSQL.csv\", \"NetBIOS.csv\", \"Portmap.csv\", \"Syn.csv\"]\n# Uncomment if fixed\n# files += [\"UDP.csv\", \"UDPLag.csv\"]  \n\n# Load first file\nflows = load_file(base_path + files[0])  # Expecting ONE DataFrame\nprint('file 1 loaded')\n\n# Load remaining files\nfor i, file in enumerate(files[1:], start=2):\n    df = load_file(base_path + file)  # Expecting ONE DataFrame\n    \n    # Concatenate the new file data\n    flows = pd.concat([flows, df], ignore_index=True)\n\n    print(f'file {i} loaded')\n\n# Save to CSV\nflows.to_csv('/kaggle/working/export_tests.csv', index=False, header=True)\n\n# Free memory\ndel flows, df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:18:33.888616Z","iopub.execute_input":"2025-05-11T12:18:33.889048Z","iopub.status.idle":"2025-05-11T12:20:53.268466Z","shell.execute_reply.started":"2025-05-11T12:18:33.889005Z","shell.execute_reply":"2025-05-11T12:20:53.263810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CIC-DDoS2019 Data Processing**\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport hashlib\n\n# Load dataset\nsamples = pd.read_csv('/kaggle/working/export_dataframe.csv', sep=',')\n\n# Function to convert string to numeric hash\ndef string2numeric_hash(text):\n    return int(hashlib.md5(text.encode()).hexdigest()[:8], 16)\n\n# Replace infinite values\nsamples = samples.replace(['Infinity', np.inf], 0)\n\n# Convert numerical columns safely\nsamples[' Flow Packets/s'] = pd.to_numeric(samples[' Flow Packets/s'], errors='coerce').fillna(0)\nsamples['Flow Bytes/s'] = pd.to_numeric(samples['Flow Bytes/s'], errors='coerce').fillna(0)\n\n# Convert labels to numeric\nsamples[' Label'] = samples[' Label'].replace({\n    'BENIGN': 0, 'DrDoS_DNS': 1, 'DrDoS_LDAP': 1, 'DrDoS_MSSQL': 1,\n    'DrDoS_NTP': 1, 'DrDoS_NetBIOS': 1, 'DrDoS_SNMP': 1, 'DrDoS_SSDP': 1,\n    'DrDoS_UDP': 1, 'Syn': 1, 'TFTP': 1, 'UDP-lag': 1, 'WebDDoS': 1\n}).astype(int)\n\n# Ensure no NaN timestamps before splitting\nsamples[' Timestamp'] = samples[' Timestamp'].fillna('1970-01-01 00:00:00.000000')\n\n# Process timestamps\ncolumnTime = samples[' Timestamp'].str.split(' ', n=1, expand=True)\ncolumnTime.columns = ['day', 'time']\ncolumnTime = columnTime['time'].str.split('.', n=1, expand=True)\ncolumnTime.columns = ['time', 'milliseconds']\nsamples[' Timestamp'] = columnTime['time'].apply(string2numeric_hash)\n\n# Drop unnecessary columns\nsamples.drop(columns=[' Source IP', ' Destination IP', 'Flow ID', 'SimillarHTTP', 'Unnamed: 0'], inplace=True)\n\n# Save processed dataset\nsamples.to_csv('/kaggle/working/export_dataframe_proc.csv', index=False, header=True)\n\nprint('Training data processed successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:20:53.269757Z","iopub.execute_input":"2025-05-11T12:20:53.270033Z","iopub.status.idle":"2025-05-11T12:20:53.434280Z","shell.execute_reply.started":"2025-05-11T12:20:53.270009Z","shell.execute_reply":"2025-05-11T12:20:53.433245Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport hashlib# Processing test dataset\ntests = pd.read_csv('/kaggle/working/export_tests.csv', sep=',')\n\n# Replace infinite values\ntests = tests.replace(['Infinity', np.inf], 0)\n\n# Convert numerical columns safely\ntests[' Flow Packets/s'] = pd.to_numeric(tests[' Flow Packets/s'], errors='coerce').fillna(0)\ntests['Flow Bytes/s'] = pd.to_numeric(tests['Flow Bytes/s'], errors='coerce').fillna(0)\n\n# Convert labels to numeric\ntests[' Label'] = tests[' Label'].replace({\n    'BENIGN': 0, 'LDAP': 1, 'NetBIOS': 1, 'MSSQL': 1,\n    'Portmap': 1, 'Syn': 1\n}).astype(int)\n\n# Ensure no NaN timestamps before splitting\ntests[' Timestamp'] = tests[' Timestamp'].fillna('1970-01-01 00:00:00.000000')\n\n# Process timestamps\ncolumnTime = tests[' Timestamp'].str.split(' ', n=1, expand=True)\ncolumnTime.columns = ['day', 'time']\ncolumnTime = columnTime['time'].str.split('.', n=1, expand=True)\ncolumnTime.columns = ['time', 'milliseconds']\ntests[' Timestamp'] = columnTime['time'].apply(string2numeric_hash)\n\n# Drop unnecessary columns\ntests.drop(columns=[' Source IP', ' Destination IP', 'Flow ID', 'SimillarHTTP', 'Unnamed: 0'], inplace=True)\n\n# Save processed dataset\ntests.to_csv('/kaggle/working/export_tests_proc.csv', index=False, header=True)\n\nprint('Test data processed successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:20:53.435392Z","iopub.execute_input":"2025-05-11T12:20:53.435636Z","iopub.status.idle":"2025-05-11T12:20:53.543014Z","shell.execute_reply.started":"2025-05-11T12:20:53.435617Z","shell.execute_reply":"2025-05-11T12:20:53.541787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:20:53.544504Z","iopub.execute_input":"2025-05-11T12:20:53.544882Z","iopub.status.idle":"2025-05-11T12:21:06.630563Z","shell.execute_reply.started":"2025-05-11T12:20:53.544848Z","shell.execute_reply":"2025-05-11T12:21:06.629808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DNN","metadata":{}},{"cell_type":"code","source":"def DNN_model(input_size):\n   \n    # Initialize the constructor\n    model = Sequential()\n    \n    model.add(Dense(2, activation='relu', input_shape=(input_size,)))\n    #model.add(Dense(100, activation='relu'))   \n    #model.add(Dense(40, activation='relu'))\n    #model.add(Dense(10, activation='relu'))\n    #model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    print(model.summary())\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:21:06.633878Z","iopub.execute_input":"2025-05-11T12:21:06.634409Z","iopub.status.idle":"2025-05-11T12:21:06.639341Z","shell.execute_reply.started":"2025-05-11T12:21:06.634382Z","shell.execute_reply":"2025-05-11T12:21:06.638386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nSupport Vector Machine (SVM)","metadata":{}},{"cell_type":"code","source":"def SVM():\n    return SVC(kernel='linear')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:21:06.641006Z","iopub.execute_input":"2025-05-11T12:21:06.641361Z","iopub.status.idle":"2025-05-11T12:21:06.733012Z","shell.execute_reply.started":"2025-05-11T12:21:06.641326Z","shell.execute_reply":"2025-05-11T12:21:06.732023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Auxiliar Functions\n\nImplementation of auxiliar functions, such as testing, compiling/training, 3d reshape, etc.\ntrain_test(samples)\n\n    Receives a group of samples and split it in train/test sets.","metadata":{}},{"cell_type":"code","source":"def train_test(samples, test_size=0.33):\n    from sklearn.model_selection import train_test_split\n    X = samples.iloc[:, :-1]\n    y = samples.iloc[:, -1]\n    return train_test_split(X, y, test_size=test_size, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:47:39.090347Z","iopub.execute_input":"2025-05-11T12:47:39.090661Z","iopub.status.idle":"2025-05-11T12:47:39.096498Z","shell.execute_reply.started":"2025-05-11T12:47:39.090637Z","shell.execute_reply":"2025-05-11T12:47:39.095497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_data(X_train,X_test):\n    # Import `StandardScaler` from `sklearn.preprocessing`\n    from sklearn.preprocessing import StandardScaler,MinMaxScaler\n    \n    # Define the scaler \n    #scaler = StandardScaler().fit(X_train)\n    scaler = MinMaxScaler(feature_range=(-1, 1)).fit(X_train)\n    \n    # Scale the train set\n    X_train = scaler.transform(X_train)\n    \n    # Scale the test set\n    X_test = scaler.transform(X_test)\n    \n    return X_train, X_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:47:44.717796Z","iopub.execute_input":"2025-05-11T12:47:44.718143Z","iopub.status.idle":"2025-05-11T12:47:44.722676Z","shell.execute_reply.started":"2025-05-11T12:47:44.718113Z","shell.execute_reply":"2025-05-11T12:47:44.721799Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\ncompile_train(model,X_train,y_train,deep=True)¶\n\n    Compile and train learning model\n\n    deep = False for scikit-learn ML methods\n\n","metadata":{}},{"cell_type":"code","source":"def compile_train(model, X_train, y_train, epochs=10, batch_size=64, deep=True):\n    \"\"\"\n    Compile and train the learning model.\n    \"\"\"\n    if deep:\n        import matplotlib.pyplot as plt\n        model.compile(loss='binary_crossentropy',\n                      optimizer='adam',\n                      metrics=['accuracy'])\n        history = model.fit(\n            X_train, y_train,\n            epochs=epochs,\n            batch_size=batch_size,\n            verbose=1\n        )\n        # plotting...\n        plt.plot(history.history['accuracy'])\n        plt.title('Model Accuracy'); plt.show()\n        plt.plot(history.history['loss'])\n        plt.title('Model Loss');     plt.show()\n        print('Metrics:', model.metrics_names)\n    else:\n        model.fit(X_train, y_train)\n\n    print('Model Compiled and Trained')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:48:17.289870Z","iopub.execute_input":"2025-05-11T12:48:17.290202Z","iopub.status.idle":"2025-05-11T12:48:17.296744Z","shell.execute_reply.started":"2025-05-11T12:48:17.290169Z","shell.execute_reply":"2025-05-11T12:48:17.295638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def testes(model,X_test,y_test,y_pred, deep=True):\n    if(deep==True): \n        score = model.evaluate(X_test, y_test,verbose=1)\n\n        print(score)\n    \n    # Alguns testes adicionais\n    #y_test = formatar2d(y_test)\n    #y_pred = formatar2d(y_pred)\n    \n    \n    # Import the modules from `sklearn.metrics`\n    from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score, accuracy_score\n    \n    # Accuracy \n    acc = accuracy_score(y_test, y_pred)\n    print('\\nAccuracy')\n    print(acc)\n    \n    # Precision \n    prec = precision_score(y_test, y_pred)#,average='macro')\n    print('\\nPrecision')\n    print(prec)\n    \n    # Recall\n    rec = recall_score(y_test, y_pred) #,average='macro')\n    print('\\nRecall')\n    print(rec)\n    \n    # F1 score\n    f1 = f1_score(y_test,y_pred) #,average='macro')\n    print('\\nF1 Score')\n    print(f1)\n    \n    #average\n    avrg = (acc+prec+rec+f1)/4\n    print('\\nAverage (acc, prec, rec, f1)')\n    print(avrg)\n    \n    return acc, prec, rec, f1, avrg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:48:26.071332Z","iopub.execute_input":"2025-05-11T12:48:26.071650Z","iopub.status.idle":"2025-05-11T12:48:26.077756Z","shell.execute_reply.started":"2025-05-11T12:48:26.071625Z","shell.execute_reply":"2025-05-11T12:48:26.076806Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Calculate the correct classification rate of normal and attack flow records","metadata":{}},{"cell_type":"code","source":"def test_normal_atk(y_test, y_pred):\n    import pandas as pd\n\n    df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n    # общее число нормальных и атак\n    normal = (df['y_test'] == 0).sum()\n    atk    = (df['y_test'] == 1).sum()\n    \n    # ошибки по классам\n    wrong = df[df['y_test'] != df['y_pred']]\n    counts = wrong.groupby('y_test').size().to_dict()\n    \n    # сколько ошибок у каждого класса (0 или 1)\n    wrong_normal = counts.get(0, 0)\n    wrong_atk    = counts.get(1, 0)\n    \n    normal_detect_rate = (normal - wrong_normal) / normal if normal > 0 else 0\n    atk_detect_rate    = (atk    - wrong_atk)    / atk    if atk    > 0 else 0\n    \n    return normal_detect_rate, atk_detect_rate\nprint(\"123\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:48:30.259275Z","iopub.execute_input":"2025-05-11T12:48:30.259627Z","iopub.status.idle":"2025-05-11T12:48:30.265654Z","shell.execute_reply.started":"2025-05-11T12:48:30.259596Z","shell.execute_reply":"2025-05-11T12:48:30.264405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Methods for saving and loading trained models","metadata":{}},{"cell_type":"code","source":"def save_model(model,name):\n    from keras.models import model_from_json\n    \n    arq_json = 'Models/' + name + '.json'\n    model_json = model.to_json()\n    with open(arq_json,\"w\") as json_file:\n        json_file.write(model_json)\n    \n    arq_h5 = 'Models/' + name + '.h5'\n    model.save_weights(arq_h5)\n    print('Model Saved')\n    \ndef load_model(name):\n    from keras.models import model_from_json\n    \n    arq_json = 'Models/' + name + '.json'\n    json_file = open(arq_json,'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    loaded_model = model_from_json(loaded_model_json)\n    \n    arq_h5 = 'Models/' + name + '.h5'\n    loaded_model.load_weights(arq_h5)\n    \n    print('Model loaded')\n    \n    return loaded_model\n\ndef save_Sklearn(model,nome):\n    import pickle\n    arquivo = 'Models/'+ nome + '.pkl'\n    with open(arquivo,'wb') as file:\n        pickle.dump(model,file)\n    print('Model sklearn saved')\n\ndef load_Sklearn(nome):\n    import pickle\n    arquivo = 'Models/'+ nome + '.pkl'\n    with open(arquivo,'rb') as file:\n        model = pickle.load(file)\n    print('Model sklearn loaded')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:48:33.359334Z","iopub.execute_input":"2025-05-11T12:48:33.359675Z","iopub.status.idle":"2025-05-11T12:48:33.367191Z","shell.execute_reply.started":"2025-05-11T12:48:33.359645Z","shell.execute_reply":"2025-05-11T12:48:33.366059Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"samples = pd.read_csv('/kaggle/working/export_dataframe_proc.csv', sep=',')\n\nX_tmp, _, y_tmp, _ = train_test(samples, test_size=0.33)\nX_ap = pd.concat([X_tmp, y_tmp], axis=1)\n\nis_benign = X_ap[' Label'] == 0\nnormal    = X_ap[is_benign]\nddos      = X_ap[~is_benign]\n\nnormal_upsampled = resample(normal, replace=True, n_samples=len(ddos), random_state=27)\nupsampled_df     = pd.concat([normal_upsampled, ddos], ignore_index=True)\n\n# Опционально массивы, но дальше работаем с upsampled_df\nX_train_base = upsampled_df.iloc[:, :-1].values\ny_train_base = upsampled_df.iloc[:, -1].values\n\ndel X_tmp, y_tmp, X_ap, normal, ddos, normal_upsampled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:48:40.178579Z","iopub.execute_input":"2025-05-11T12:48:40.178953Z","iopub.status.idle":"2025-05-11T12:48:40.209355Z","shell.execute_reply.started":"2025-05-11T12:48:40.178923Z","shell.execute_reply":"2025-05-11T12:48:40.207966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# Предполагаем, что upsampled_df определён выше,\n# а X_test_base, y_test_base — тоже.\n\nresults = pd.DataFrame(columns=[\n    'TestSize','BatchSize','Epochs','Method',\n    'Accuracy','Precision','Recall','F1_Score',\n    'Normal_Detect_Rate','Atk_Detect_Rate'\n])\n\nfor test_size in [0.2, 0.3, 0.4]:\n    # 1) Разбиение апсемплингового DataFrame\n    X_tr, X_val, y_tr, y_val = train_test(upsampled_df, test_size=test_size)\n    # 2) Нормализация\n    X_tr, X_val = normalize_data(X_tr, X_val)\n\n    # DNN: перебор batch_size и epochs\n    for batch_size in [32, 64, 128]:\n        for epochs in [10, 15, 20, 25, 30]:\n            model = DNN_model(X_tr.shape[1])\n            model = compile_train(\n                model, X_tr, y_tr,\n                epochs=epochs,\n                batch_size=batch_size,\n                deep=True\n            )\n\n            # предсказания\n            y_pred = model.predict(X_val).round()\n            acc, prec, rec, f1, _ = testes(model, X_val, y_val, y_pred)\n            y_pred_flat = y_pred.ravel()  # или y_pred.flatten()\n            norm_r, atk_r = test_normal_atk(y_val, y_pred_flat)\n            # визуализация CM\n            disp = ConfusionMatrixDisplay.from_predictions(\n                y_val, y_pred,\n                display_labels=['Benign','DDoS']\n            )\n            plt.title(f'DNN CM: ts={test_size}, bs={batch_size}, ep={epochs}')\n            plt.show()\n\n            # сохраняем метрики\n            results = pd.concat([results, pd.DataFrame([{\n                'TestSize': test_size,\n                'BatchSize': batch_size,\n                'Epochs': epochs,\n                'Method': 'DNN',\n                'Accuracy': acc,\n                'Precision': prec,\n                'Recall': rec,\n                'F1_Score': f1,\n                'Normal_Detect_Rate': norm_r,\n                'Atk_Detect_Rate': atk_r\n            }])], ignore_index=True)\n\n    # SVM\n    model_svm = SVM()\n    model_svm.fit(X_tr, y_tr)\n    y_pred_svm = model_svm.predict(X_val)\n    y_pred_svm = y_pred_svm.ravel()\n    acc_s, prec_s, rec_s, f1_s, _ = testes(model_svm, X_val, y_val, y_pred_svm, deep=False)\n    norm_s, atk_s = test_normal_atk(y_val, y_pred_svm)\n\n    # визуализация CM для SVM\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_val, y_pred_svm,\n        display_labels=['Benign','DDoS']\n    )\n    plt.title(f'SVM CM: ts={test_size}')\n    plt.show()\n\n    # сохраняем метрики SVM\n    results = pd.concat([results, pd.DataFrame([{\n        'TestSize': test_size,\n        'BatchSize': None,\n        'Epochs': None,\n        'Method': 'SVM',\n        'Accuracy': acc_s,\n        'Precision': prec_s,\n        'Recall': rec_s,\n        'F1_Score': f1_s,\n        'Normal_Detect_Rate': norm_s,\n        'Atk_Detect_Rate': atk_s\n    }])], ignore_index=True)\n\n# Сохраняем финальную таблицу\nresults.to_csv('/kaggle/working/experiment_results.csv', index=False)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:21:07.105460Z","iopub.status.idle":"2025-05-11T12:21:07.105791Z","shell.execute_reply":"2025-05-11T12:21:07.105640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nresults = pd.read_csv('/kaggle/working/experiment_results.csv')\n#по убыванию Accuracy\nsorted_results = results.sort_values(by='Accuracy', ascending=False)\n#Сбросим индексы для красоты\nsorted_results = sorted_results.reset_index(drop=True)\ndisplay(sorted_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:21:07.106399Z","iopub.status.idle":"2025-05-11T12:21:07.106676Z","shell.execute_reply":"2025-05-11T12:21:07.106565Z"}},"outputs":[],"execution_count":null}]}